{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Final Code :"
      ],
      "metadata": {
        "id": "EUkJ9ApX15WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall googletrans\n",
        "!pip install googletrans==3.1.0a0\n",
        "import googletrans\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tag import tnt\n",
        "from nltk.corpus import indian\n",
        "from googletrans import Translator\n",
        "\n",
        "# Download the Hindi POS tagger\n",
        "nltk.download('indian')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "Ohvr_eHREfei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d685a937-a746-4d5a-b4e1-6dbc209376cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.12.7)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=72e02f59197f6e8dffbe715e57ebce5b473997f28c754940a8eb0904c253b1c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/59/af/8d6c96a719763990f1c548e36b17d9efdfb767f42f7ff39f53\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/indian.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "sentence_id = 0\n",
        "\n",
        "text = \"गर्मी के एक दिन में, जंगल के एक शेर को बहुत जोरों से भूख लगी. इसलिए वो इधर उधर खाने की तलाश करने लगा. कुछ देर खोजने के बाद उसे एक खरगोश मिला, लेकिन उसे खाने के बदले में उसे उसने छोड़ दिया क्यूंकि उसे वो बहुत ही छोटा लगा.\"\n",
        "model_path = \"/content/hindi.pos\" #Copy hindi.pos from NLTK corpus\n",
        "\n",
        "def train_hindi_model(model_path):\n",
        "    train_data = indian.tagged_sents(model_path)\n",
        "    tnt_pos_tagger = tnt.TnT()\n",
        "    tnt_pos_tagger.train(train_data)\n",
        "    return tnt_pos_tagger\n",
        "\n",
        "\n",
        "def get_sentId(model_path):\n",
        "    ids = re.compile('<Sentence\\sid=\\d+>')\n",
        "    with open(model_path, \"r+\") as temp_f:\n",
        "        content = temp_f.readlines()\n",
        "        for i in content:\n",
        "            id_found = (ids.findall(i))\n",
        "            if id_found:\n",
        "                id_found = str(id_found).replace(\"['<Sentence id=\", \"\").replace(\">']\", \"\")\n",
        "                id = int(id_found)\n",
        "    id = id + 1\n",
        "    return id\n",
        "\n",
        "\n",
        "def tag_words(model,text):\n",
        "    tagged = (model.tag(nltk.word_tokenize(text)))\n",
        "    return tagged\n",
        "\n",
        "\n",
        "def handle_UNK(tagged_words, model_path, sentence_id):\n",
        "    with open(model_path, \"r+\") as f1:\n",
        "        result_list = []\n",
        "        for nep_word, tag in tagged_words:\n",
        "            if tag == \"Unk\":\n",
        "                x = translator.translate(nep_word)\n",
        "                if x is not None:\n",
        "                    str1 = str(x)\n",
        "                    new_str = str1.split()\n",
        "                    for j in new_str:\n",
        "                        if re.search('^text=', j, re.I):\n",
        "                            word = j.replace(\"text=\", \",\").replace(\",\", \"\")\n",
        "                            word = str(word)\n",
        "                            # pos=nltk.pos_tag(word)\n",
        "                            pos = nltk.tag.pos_tag([word])\n",
        "                            # print (i, pos)\n",
        "                            for en_word, tag in pos:\n",
        "                                result = nep_word + \"_\" + (tag) + \" \"\n",
        "                                result_list.append(result)\n",
        "\n",
        "            else:\n",
        "                result = nep_word + \"_\" + (tag) + \" \"\n",
        "                result_list.append(result)\n",
        "\n",
        "        writing_word = str(\"\\n<Sentence id=\") + str(sentence_id) + \">\\n\"\n",
        "        output = writing_word + \"\".join(result_list) + \"\\n</Sentence>\\n</Corpora>\"\n",
        "        for line in f1.readlines():\n",
        "            f1.write(line.replace(\"</Corpora>\", \"\"))\n",
        "        f1.write(output)\n",
        "\n",
        "\n",
        "sentence_id = (get_sentId(model_path))\n",
        "print (sentence_id)\n",
        "\n",
        "model = train_hindi_model(model_path)\n",
        "tagged_words = tag_words(model,text)\n",
        "\n",
        "print (\"=================================Tagged words=================================\\n\",tagged_words,\"\\n\")\n",
        "\n",
        "handle_UNK(tagged_words,model_path,sentence_id)\n",
        "\n",
        "#retrain the model\n",
        "model = train_hindi_model(model_path)\n",
        "new_tagged_words =  tag_words(model,text)\n",
        "print (\"=================================New Tagged words=================================\\n\",new_tagged_words,\"\\n\")"
      ],
      "metadata": {
        "id": "CjgcYlDZqy87",
        "outputId": "8efb83c5-05c2-40ce-ed94-717f9b831cd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "542\n",
            "=================================Tagged words=================================\n",
            " [('गर्मी', 'Unk'), ('के', 'PREP'), ('एक', 'QFNUM'), ('दिन', 'NN'), ('में', 'PREP'), (',', 'PUNC'), ('जंगल', 'Unk'), ('के', 'PREP'), ('एक', 'QFNUM'), ('शेर', 'Unk'), ('को', 'PREP'), ('बहुत', 'INTF'), ('जोरों', 'NN'), ('से', 'PREP'), ('भूख', 'Unk'), ('लगी', 'VFM'), ('.', 'SYM'), ('इसलिए', 'RB'), ('वो', 'PRP'), ('इधर', 'Unk'), ('उधर', 'RB'), ('खाने', 'Unk'), ('की', 'PREP'), ('तलाश', 'NN'), ('करने', 'VNN'), ('लगा', 'VFM'), ('.', 'SYM'), ('कुछ', 'QF'), ('देर', 'Unk'), ('खोजने', 'Unk'), ('के', 'PREP'), ('बाद', 'PREP'), ('उसे', 'PRP'), ('एक', 'QFNUM'), ('खरगोश', 'Unk'), ('मिला', 'VFM'), (',', 'PUNC'), ('लेकिन', 'CC'), ('उसे', 'PRP'), ('खाने', 'Unk'), ('के', 'PREP'), ('बदले', 'NN'), ('में', 'PREP'), ('उसे', 'PRP'), ('उसने', 'PRP'), ('छोड़', 'Unk'), ('दिया', 'VAUX'), ('क्यूंकि', 'Unk'), ('उसे', 'PRP'), ('वो', 'PRP'), ('बहुत', 'INTF'), ('ही', 'RP'), ('छोटा', 'Unk'), ('लगा', 'VFM'), ('.', 'SYM')] \n",
            "\n",
            "=================================New Tagged words=================================\n",
            " [('गर्मी', 'NN'), ('के', 'PREP'), ('एक', 'QFNUM'), ('दिन', 'NN'), ('में', 'PREP'), (',', 'PUNC'), ('जंगल', 'JJS'), ('के', 'PREP'), ('एक', 'QFNUM'), ('शेर', 'NN'), ('को', 'PREP'), ('बहुत', 'INTF'), ('जोरों', 'NN'), ('से', 'PREP'), ('भूख', 'NN'), ('लगी', 'VFM'), ('.', 'SYM'), ('इसलिए', 'RB'), ('वो', 'PRP'), ('इधर', 'RB'), ('उधर', 'RB'), ('खाने', 'TO'), ('की', 'PREP'), ('तलाश', 'NN'), ('करने', 'VNN'), ('लगा', 'VFM'), ('.', 'SYM'), ('कुछ', 'QF'), ('देर', 'RB'), ('खोजने', 'VB'), ('के', 'PREP'), ('बाद', 'PREP'), ('उसे', 'PRP'), ('एक', 'QFNUM'), ('खरगोश', 'NN'), ('मिला', 'VFM'), (',', 'PUNC'), ('लेकिन', 'CC'), ('उसे', 'PRP'), ('खाने', 'TO'), ('के', 'PREP'), ('बदले', 'NN'), ('में', 'PREP'), ('उसे', 'PRP'), ('उसने', 'PRP'), ('छोड़', 'VB'), ('दिया', 'VAUX'), ('क्यूंकि', 'IN'), ('उसे', 'PRP'), ('वो', 'PRP'), ('बहुत', 'INTF'), ('ही', 'RP'), ('छोटा', 'NN'), ('लगा', 'VFM'), ('.', 'SYM')] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Model (Previous Model for Comparison) :"
      ],
      "metadata": {
        "id": "UtKJVwjkmEAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import indian\n",
        "from nltk.tag import tnt\n",
        "\n",
        "# Download the Hindi POS tagger\n",
        "nltk.download('indian')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "XKpO3kwLeZfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c062cc-7226-44a2-bb3b-0d04b4a945af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Package indian is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7PvfuqdcnMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5e52e3-eb6c-4840-8e03-ab67d721b35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('गर्मी', 'Unk'), ('के', 'PREP'), ('एक', 'QFNUM'), ('दिन', 'NN'), ('में', 'PREP'), (',', 'PUNC'), ('जंगल', 'Unk'), ('के', 'PREP'), ('एक', 'QFNUM'), ('शेर', 'Unk'), ('को', 'PREP'), ('बहुत', 'INTF'), ('जोरों', 'NN'), ('से', 'PREP'), ('भूख', 'Unk'), ('लगी', 'VFM'), ('.', 'SYM'), ('इसलिए', 'RB'), ('वो', 'Unk'), ('इधर', 'Unk'), ('उधर', 'RB'), ('खाने', 'Unk'), ('की', 'PREP'), ('तलाश', 'NN'), ('करने', 'VNN'), ('लगा', 'VFM'), ('.', 'SYM'), ('कुछ', 'QF'), ('देर', 'Unk'), ('खोजने', 'Unk'), ('के', 'PREP'), ('बाद', 'PREP'), ('उसे', 'PRP'), ('एक', 'QFNUM'), ('खरगोश', 'Unk'), ('मिला', 'VFM'), (',', 'PUNC'), ('लेकिन', 'CC'), ('उसे', 'PRP'), ('खाने', 'Unk'), ('के', 'PREP'), ('बदले', 'NN'), ('में', 'PREP'), ('उसे', 'PRP'), ('उसने', 'PRP'), ('छोड़', 'Unk'), ('दिया', 'VAUX'), ('क्यूंकि', 'Unk'), ('उसे', 'PRP'), ('वो', 'Unk'), ('बहुत', 'INTF'), ('ही', 'RP'), ('छोटा', 'Unk'), ('लगा', 'VFM'), ('.', 'SYM')]\n"
          ]
        }
      ],
      "source": [
        "# Train the POS tagger\n",
        "data = indian.tagged_sents('hindi.pos')\n",
        "tnt_pos_tagger = tnt.TnT()\n",
        "tnt_pos_tagger.train(data)\n",
        "\n",
        "# Define a function to tag Hindi text\n",
        "def tag_hindi_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Tag the tokens\n",
        "    tagged_tokens = tnt_pos_tagger.tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "# Test the function\n",
        "text = \"गर्मी के एक दिन में, जंगल के एक शेर को बहुत जोरों से भूख लगी. इसलिए वो इधर उधर खाने की तलाश करने लगा. कुछ देर खोजने के बाद उसे एक खरगोश मिला, लेकिन उसे खाने के बदले में उसे उसने छोड़ दिया क्यूंकि उसे वो बहुत ही छोटा लगा.\"\n",
        "tagged_text = tag_hindi_text(text)\n",
        "print(tagged_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_feNZLHlLCuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}